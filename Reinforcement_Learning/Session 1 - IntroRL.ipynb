{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1KV5maSGtU9"
      },
      "source": [
        "# <center> Introduction to Reinforcement Learning</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDedkNiYGtU_"
      },
      "source": [
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96yRlg56GtU_"
      },
      "source": [
        "#### Let us first make sure that all the required dependencies are installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2yWugvjGtU_"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Tobias-Fischer/RVSS2022.git"
      ],
      "metadata": {
        "id": "vwGGniauG9RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo1UIMwFGtVA"
      },
      "source": [
        "#### Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awbK78G1GtVA"
      },
      "outputs": [],
      "source": [
        "# %matplotlib notebook\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, os.path.abspath('RVSS2022/Reinforcement_Learning/Support'))\n",
        "\n",
        "from gym_simple_gridworlds.envs.grid_env import GridEnv\n",
        "from gym_simple_gridworlds.envs.grid_2dplot import *\n",
        "from gym_simple_gridworlds.helper import *\n",
        "\n",
        "from IPython.core.display import display, HTML, Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVcq_PYIGtVB"
      },
      "source": [
        "# 2. Elements of an MDP (Grid World Example)\n",
        "\n",
        "Recall the grid in which our robot lives\n",
        "\n",
        "![](https://raw.githubusercontent.com/Tobias-Fischer/RVSS2022/main/Reinforcement_Learning/Support/images/GridWorldExample.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- The states $s \\in \\mathcal{S}$ correspond to locations in the grid. Each location has also a cell index associated to it, e.g., cell index 4 is associated to location (row=1,col=0)\n",
        "- The robot can move up, down, left, or right. Actions correpond to unit increments or decrements in the specified direction.\n",
        "    - Up : (-1,0)\n",
        "    - Down: (1,0)\n",
        "    - Left: (0,-1)\n",
        "    - Right: (0, 1)\n",
        "- Each action is represented by a number. Action (Up) is represented by 0, (Down) by 1, (Left) by 2 and, finally, (Right) by 3. No actions are available at a terminal state\n",
        "\n",
        "We have defined the class ``GridEnv`` to represent our Grid World MDP. **Take a look at the attributes of this class by placing the cursor somewhere on the class' name and hit SHIFT+TAB. If there's a + button at the top of the popup tooltip, this means the documentation spans a few lines, click it to show the full docstring, then scroll up.**"
      ],
      "metadata": {
        "id": "gLPvOIGiMbl5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3oqJhvwGtVB"
      },
      "source": [
        "## 2.1 Create Environment and Explore its Attributes\n",
        "\n",
        "The noise parameter corresponds to the probability of a change of direction when an action is taken (e.g., going left/right when agent decides to move up/down)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdO2o41sGtVC"
      },
      "outputs": [],
      "source": [
        "# Create a Grid World instance\n",
        "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=-0.04)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4scjuAzGtVC"
      },
      "source": [
        "### State and Action Spaces\n",
        "\n",
        "Let's take a look at the state and action spaces of our environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed2JLqb5GtVC"
      },
      "outputs": [],
      "source": [
        "# State (or observation) space\n",
        "print(grid_world.observation_space)\n",
        "print(grid_world.get_states())\n",
        "print()\n",
        "\n",
        "# Action space\n",
        "print(grid_world.action_space)\n",
        "print(grid_world.get_actions())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4A6gWLd-GtVC"
      },
      "source": [
        "### Transition Function\n",
        "\n",
        "Let's take a look at the current state transition function. Some things to keep in mind regarding the transition function:\n",
        "\n",
        "1. Given that $\\mathcal{T}: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow \\mathbb{R}$, the ``state_transitions`` attribute of the class ``GridEnv`` corresponds to a 3-Dimensional numpy array of size $11\\times4\\times11$.\n",
        "2. With a noise attribute set to 0.2, at state 5, if the agent chooses to move up, it will end up at:\n",
        "    - state 2 with $80\\%$ probability,\n",
        "    - state 6 with $10\\%$ probability, or\n",
        "    - state 5 with $10\\%$ probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p168neVEGtVC"
      },
      "outputs": [],
      "source": [
        "print(grid_world.state_transitions[5,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzE47q6YGtVD"
      },
      "source": [
        "### Living Reward and Reward Function\n",
        "\n",
        "Let's now take a quick look at the living reward (i.e., running cost) and reward function $\\mathcal{R}: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$.\n",
        "\n",
        "1. Living reward corresponds to the attribute ``living_rewards`` of the class ``GridEnv`` and is represented as an 1-Dimensional numpy array\n",
        "2. The reward function corresponds to the attribute ``rewards`` of the class ``GridEnv`` and is also represented as a 2-Dimensional numpy array of size $11\\times4$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbHMpqtXGtVD"
      },
      "outputs": [],
      "source": [
        "# Living rewards\n",
        "print(\"Living rewards for all states:\\n{}\\n\".format(grid_world.immediate_rewards))\n",
        "\n",
        "# Reward function, i.e., expected reward for taking action a at state s\n",
        "print(\"Reward function for all state-action pairs:\\n{}\\n\".format(grid_world.rewards))\n",
        "print(\"The expected reward at state 5 if agent chooses to move right is: {}\".format(grid_world.rewards[5,3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_VKxox2GtVD"
      },
      "source": [
        "### Policy\n",
        "\n",
        "Let's see the path and total reward of an agent moving on our grid world according to the following policy $\\pi$\n",
        "\n",
        "![](https://raw.githubusercontent.com/Tobias-Fischer/RVSS2022/main/Reinforcement_Learning/Support/images/example_policy.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xv6VtarzGtVD"
      },
      "outputs": [],
      "source": [
        "# We represent this policy as a 2-Dimensional numpy array\n",
        "policy_matrix = np.array([[3,      3,  3,  -1],\n",
        "                          [0, np.NaN,  0,  -1],\n",
        "                          [0,      2,  0,   2]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6ZwCb6NGtVE"
      },
      "outputs": [],
      "source": [
        "print(grid_world.grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot4aPblnGtVE"
      },
      "source": [
        "Let's now apply this policy and observe the agent's behavior (blue dot in the figure shown below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf-1PeP7GtVE"
      },
      "outputs": [],
      "source": [
        "# Create a Grid World instance\n",
        "grid_world = GridEnv(gamma=0.99, noise=0.2, living_reward=-0.04)\n",
        "s_x, s_y = get_state_to_plot(grid_world)\n",
        "\n",
        "# We can visualize our grid world using the render() function\n",
        "fig, ax = grid_world.render()\n",
        "agent, = ax.plot([], [], 'o', color='b', linewidth=6)\n",
        "reward_text = ax.text(0.02, 0.95, '', transform=ax.transAxes)\n",
        "\n",
        "done = False\n",
        "cumulative_reward = 0\n",
        "cur_state = grid_world.cur_state\n",
        "path_to_plot = []\n",
        "\n",
        "while not done:\n",
        "    _, cur_reward, done, _ = grid_world.step(int(policy_matrix[cur_state[0], cur_state[1]]))\n",
        "    cur_state = grid_world.cur_state\n",
        "    n_x, n_y = get_state_to_plot(grid_world)\n",
        "    cumulative_reward += cur_reward\n",
        "    path_to_plot.append([cumulative_reward, n_x, n_y])\n",
        "\n",
        "def init():\n",
        "    agent.set_data(s_x + 0.5, s_y + 0.5)\n",
        "    reward_text.set_text('')\n",
        "    return agent, reward_text\n",
        "\n",
        "def animate(i):\n",
        "    if i < len(path_to_plot):\n",
        "        r, n_x, n_y = path_to_plot[i]\n",
        "        agent.set_data(n_x + 0.5, n_y + 0.5)\n",
        "        reward_text.set_text('Cumulative reward: %.2f' % r)\n",
        "    return agent, reward_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ani = animation.FuncAnimation(fig, animate, frames=len(path_to_plot), blit=False, interval=500, init_func=init,\n",
        "                              repeat=False)\n",
        "HTML(ani.to_html5_video())"
      ],
      "metadata": {
        "id": "FnFtW4CTRNGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhQHAGepGtVE"
      },
      "source": [
        "## 2.2 Summary and Code Correspondance\n",
        "\n",
        "\n",
        "<table style=\"width:20%\">\n",
        "<thead>\n",
        "<tr>\n",
        "<th style=\"width:150px;font-size:20px;text-align:center;\">MDP</th>\n",
        "<th colspan=2 style=\"width:250px;font-size:20px;text-align:center;\">GridEnv</th>\n",
        "</tr>\n",
        "    <tr>\n",
        "        <td></td>\n",
        "        <th style=\"font-size:15px;text-align:center;\">Class attribute</th>\n",
        "        <th style=\"font-size:15px;text-align:center;\">Class method</th>\n",
        "    </tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "    <td style=\"text-align:left;\">$\\mathcal{S} = \\{0,\\dots,10\\}$</td>\n",
        "    <td style=\"text-align:left;\">observation_space</td>\n",
        "    <td style=\"text-align:left;\">get_states()</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td style=\"text-align:left;\">$\\mathcal{A} = \\{\\text{up}, \\text{down}, \\text{left}, \\text{right} \\}$</td>\n",
        "    <td style=\"text-align:left;\">action_space</td>\n",
        "    <td style=\"text-align:left;\">get_actions()</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td style=\"text-align:left;\">$\\mathcal{T}(s, a, s')$</td>\n",
        "    <td style=\"text-align:left;\">state_transitions</td>\n",
        "    <td></td>\n",
        "</tr>\n",
        "<!-- <tr>\n",
        "    <td style=\"text-align:left;\">$\\mathcal{R}(s)$</td>\n",
        "    <td style=\"text-align:left;\">immediate_rewards</td>\n",
        "    <td></td>\n",
        "</tr>-->\n",
        "    <tr>\n",
        "    <td style=\"text-align:left;\">$\\mathcal{R}(s,a)$</td>\n",
        "    <td style=\"text-align:left;\">rewards</td>\n",
        "    <td></td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hMDvZSgGtVE"
      },
      "source": [
        "# 3. Iterative Policy Evaluation\n",
        "\n",
        "Recall the definition of the iterative policy evaluation algorithm\n",
        "\n",
        "![](https://raw.githubusercontent.com/Tobias-Fischer/RVSS2022/main/Reinforcement_Learning/Support/images/IterativePolicyEvaluation.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now compute the value function of the same policy $\\pi$\n",
        "\n",
        "![](https://raw.githubusercontent.com/Tobias-Fischer/RVSS2022/main/Reinforcement_Learning/Support/images/example_policy.png)"
      ],
      "metadata": {
        "id": "EAuWEUj8NKrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We consider a grid world environment with the following attributes:\n",
        "- Discount factor $\\gamma = 0.99$ (class attribute ``gamma=0.99``)\n",
        "- Stochastic transition matrix (class attribute ``noise=0.2``)\n",
        "- A non-zero living cost and big rewards are obtained at terminal states (class attribute ``living_reward=-0.04``)\n",
        "\n",
        "We have defined the helper function ``encode_policy()`` to encode the policy $\\pi$ shown in the image above. The return variable ``policy_pi`` is a dictionary of dictionaries, where each element corresponds to the probability of selecting an action $a$ at a given state $s$\n",
        "\n",
        "Keep in mind that each action is represented by a number. Action (Up) is represented by 0, (Down) by 1, (Left) by 2 and, finally, (Right) by 3."
      ],
      "metadata": {
        "id": "t40A0zgbNK9Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMy1n30nGtVF"
      },
      "outputs": [],
      "source": [
        "grid_world = GridEnv(gamma=0.99, noise=0.2, living_reward=-0.04)\n",
        "policy_pi = encode_policy(grid_world)\n",
        "\n",
        "print(\"Action probabilities at state 0 are:\\n{}\".format(dict(policy_pi[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y5WupMJGtVF"
      },
      "source": [
        "Given the policy $\\pi$, let's know compute its state-value function using iterative policy evaluation.\n",
        "\n",
        "**TODO**: \n",
        "Complete the computation of value function update for each state. We have decomposed this computation into 2 steps:\n",
        "\n",
        "1. Compute discounted sum of state values of all successor states: $\\text{discounted_v} = \\gamma\\sum_{s' \\in \\mathcal{S}}\\mathcal{T}(s,a,s')v(s')$ for each action\n",
        "\n",
        "\n",
        "2. Compute expectation over all actions: $\\sum_{a \\in \\mathcal{A}}\\pi(a|s)(\\mathcal{R}(s,a) + \\text{discounted_v})$ \n",
        "\n",
        "\n",
        "**Keep in Mind**: Correspondance between the mathematical notation and implemented code\n",
        "\n",
        "\n",
        "<table style=\"width:20%\">\n",
        "<thead>\n",
        "<tr>\n",
        "<th style=\"width:150px;font-size:20px;text-align:center;\">Notation</th>\n",
        "<th colspan=2, style=\"width:450px;font-size:20px;text-align:center;\">Code</th>\n",
        "</tr>\n",
        "    <tr>\n",
        "        <td></td>\n",
        "        <th style=\"font-size:15px;text-align:center;\">Variable/Attribute</th>\n",
        "        <th style=\"font-size:15px;text-align:center;\">Type</th>\n",
        "    </tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "    <td style=\"text-align:left;\">$\\gamma$</td>\n",
        "    <td style=\"text-align:left;font-size:15px;\">grid_world.gamma</td>\n",
        "    <td>float</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td style=\"text-align:left;\">$\\mathcal{T}(s, a, s')$</td>\n",
        "    <td style=\"text-align:left;font-size:15px;\">grid_world.state_transitions[idx_s, idx_a, idx_s]</td>\n",
        "    <td>numpy 3d-array</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td style=\"text-align:left;\">$\\mathcal{R}(s, a)$</td>\n",
        "    <td style=\"text-align:left;font-size:15px;\">grid_world.rewards[idx_s, idx_a]</td>\n",
        "    <td>numpy 2d-array</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td style=\"text-align:left;\">$\\pi(a|s)$</td>\n",
        "    <td style=\"text-align:left;font-size:15px;\">policy_pi[idx_s][idx_a]</td>\n",
        "    <td>dict of dict</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td style=\"text-align:left;\">$v_\\pi(s)$</td>\n",
        "    <td style=\"text-align:left;font-size:15px;\">v[idx_s]</td>\n",
        "    <td>dict</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVu1D1gRGtVF"
      },
      "outputs": [],
      "source": [
        "def policy_evaluation(grid_env, policy, plot=False, threshold=0.00001):\n",
        "    \n",
        "    \"\"\"\n",
        "    This function computes the value function for a policy pi in a given environment grid_env.\n",
        "    \n",
        "    :param grid_env (GridEnv): MDP environment\n",
        "    :param policy (dict - stochastic form): Policy being evaluated\n",
        "    :return: (dict) State-values for all non-terminal states\n",
        "    \"\"\"\n",
        "        \n",
        "    # Obtain list of all states in environment\n",
        "    v = {s: 0.0 for s in grid_env.get_states()}\n",
        "    theta = threshold\n",
        "    delta = 1000\n",
        "\n",
        "    while delta > theta:\n",
        "        delta = 0.0\n",
        "        # For all states\n",
        "        for s in v.keys():\n",
        "\n",
        "            old_v = v[s]\n",
        "            new_v = 0\n",
        "\n",
        "            # For all actions\n",
        "            for a in grid_env.get_actions():\n",
        "                discounted_v = 0\n",
        "\n",
        "                # For all states that are reachable from s with action a\n",
        "                for s_next in grid_env.get_states():\n",
        "                    # TODO 1: Compute discounted sum of state values for all successor states\n",
        "                    discounted_v += 0\n",
        "                    \n",
        "                # TODO 2: Compute expectation over all actions\n",
        "                new_v += 0\n",
        "\n",
        "            v[s] = new_v\n",
        "            delta = max(delta, np.abs(old_v - new_v))\n",
        "\n",
        "    if plot:\n",
        "        plot_value_function(grid_env, v)\n",
        "        \n",
        "    return v\n",
        "        \n",
        "        \n",
        "# Call the policy evalution function\n",
        "v = policy_evaluation(grid_world, policy_pi, plot=True)\n",
        "print(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtO7Uk-nGtVF"
      },
      "source": [
        "# 4. Policy Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQUzao_mGtVG"
      },
      "source": [
        "Recall the definition of the policy iteration algorithm\n",
        "\n",
        "![](https://raw.githubusercontent.com/Tobias-Fischer/RVSS2022/main/Reinforcement_Learning/Support/images/PolicyIteration.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting with a random policy, let's find the optimal policy for a grid world environment with attributes:\n",
        "\n",
        "We consider a grid world environment with the following attributes:\n",
        "- Discount factor $\\gamma = 0.99$ (class attribute ``gamma=0.99``)\n",
        "- Stochastic transition matrix (class attribute ``noise=0.2``)\n",
        "- Rewards are only obtained at terminal states (class attribute ``living_reward=-0.04``)\n",
        "\n",
        "We will first define some helper methods:\n",
        "- ``one_step_look_ahead(grid_env, state, value_function)``, this method computes the action-value function for a state $s$ given the state-value function $v$. This corresponds to $\\mathcal{R}(s,a) + \\gamma\\sum_{s' \\in \\mathcal{S}}\\mathcal{T}(s,a,s')v_\\pi(s')\\, \\forall \\, a \\in \\mathcal{A}$\n",
        "\n",
        "\n",
        "- ``update_policy(grid_world, policy, value_function)``, this method updates the current policy $\\pi$ given the state-value function $v$ by taking the action $a$ with the highest action-value. \n",
        "\n",
        "\n",
        "- ``define_random_policy(grid_env)`` in script ``helper.py``, this method generates a random policy for environment ``grid_env``"
      ],
      "metadata": {
        "id": "GtDsTrEjNtsl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lgB6hMuGtVG"
      },
      "outputs": [],
      "source": [
        "def one_step_look_ahead(grid_env, state, value_function):\n",
        "    \"\"\"\n",
        "     Compute the action-value function for a state $s$ given the state-value function $v$.\n",
        "     \n",
        "     :param grid_env (GridEnv): MDP environment\n",
        "     :param state (int): state for which we are looking one action ahead\n",
        "     :param value_function (dict): state-value function associated to a given policy py\n",
        "     \n",
        "     :return: (np.array) Action-value function for all actions available at state s\n",
        "    \"\"\"\n",
        "    action_values = []\n",
        "    \n",
        "    for action in grid_env.get_actions():\n",
        "        discounted_value = 0\n",
        "        for s_next in grid_env.get_states():\n",
        "             discounted_value += grid_env.state_transitions[state, action, s_next] * value_function[s_next]\n",
        "        \n",
        "        q_a = grid_env.rewards[state, action] + grid_env.gamma * discounted_value\n",
        "        action_values.append(q_a)\n",
        "    \n",
        "    return np.array(action_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsnGxnvtGtVG"
      },
      "outputs": [],
      "source": [
        "def update_policy(grid_env, cur_policy, value_function):\n",
        "    \"\"\"\n",
        "     Update a given policy based on a given value_function\n",
        "     \n",
        "     :param grid_env (GridEnv): MDP environment\n",
        "     :param cur_policy (matrix form): Policy to update\n",
        "     :param value_function: state-value function associated to a policy cur_policy\n",
        "     \n",
        "     :return: (dict) Updated policy\n",
        "    \"\"\"\n",
        "    \n",
        "    states = grid_env.get_states(exclude_terminal=True)\n",
        "    \n",
        "    for s in states:\n",
        "        # Obtain state-action values for state s using the helper function one_step_look_ahead\n",
        "        action_values = one_step_look_ahead(grid_env, s, value_function)\n",
        "        \n",
        "        # Find (row, col) coordinates of cell with index s\n",
        "        row, col = np.argwhere(grid_env.grid == s)[0]\n",
        "        \n",
        "        cur_policy[row, col] = np.argmax(action_values)\n",
        "        \n",
        "    return cur_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4WkoHtbGtVG"
      },
      "source": [
        "Let's now define the policy iteration core algorithm.\n",
        "\n",
        "**TODO**: Complete the main steps of the policy iteration algoritm.\n",
        "- Use ``policy_evaluation(.)`` to compute the state-value function of a given policy\n",
        "- Use ``update_policy(.)`` to obtain an updated policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1XJB-z5GtVG"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(grid_env, policy, plot=False):\n",
        "    \"\"\"\n",
        "    This function iteratively updates a given policy pi for a given environment grid_env until convergence to optimal policy\n",
        "    \n",
        "    :param grid_env (GridEnv): MDP environment\n",
        "    :param policy (matrix from): Deteministic policy being updated\n",
        "    :return: (dict) State-values for all non-terminal states\n",
        "    \"\"\"\n",
        "    prev_policy = np.zeros(policy.shape)\n",
        "    \n",
        "    while not np.all(np.equal(prev_policy, policy)):\n",
        "        \n",
        "        # Encode policy. This policy representation is needed for policy evaluation\n",
        "        encoded_policy = encode_policy(grid_env, policy)\n",
        "        # Set prev_policy to current policy\n",
        "        prev_policy = policy.copy()\n",
        "        \n",
        "        #TODO: Complete the remaining steps\n",
        "        # 1. Evaluate the given policy (policy_evaluation expects an mdp and the enconded_policy as arguments)\n",
        "        # 2. Update policy using helper function update_policy\n",
        "        \n",
        "    if plot:\n",
        "        plot_policy(grid_env, policy)\n",
        "    \n",
        "    return policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFrBvPSsGtVH"
      },
      "outputs": [],
      "source": [
        "# Create a grid world mdp\n",
        "grid_world = GridEnv(gamma=0.99, noise=0.2, living_reward=-0.04)\n",
        "\n",
        "# Generate an initial random policy\n",
        "initial_policy = define_random_policy(grid_world)\n",
        "\n",
        "# Compute optimal policy using policy iteration\n",
        "optimal_policy = policy_iteration(grid_world, initial_policy, plot=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "IntelligentRobotics",
      "language": "python",
      "name": "intelligentrobotics"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "IntroRL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}