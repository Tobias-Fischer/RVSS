{"cells":[{"cell_type":"markdown","metadata":{"id":"W1KV5maSGtU9"},"source":["# <center> Introduction to Reinforcement Learning</center>"]},{"cell_type":"markdown","metadata":{"id":"KDedkNiYGtU_"},"source":["# 1. Setup"]},{"cell_type":"markdown","metadata":{"id":"96yRlg56GtU_"},"source":["#### Let us first make sure that all the required dependencies are installed"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"U2yWugvjGtU_","executionInfo":{"status":"ok","timestamp":1643770377635,"user_tz":-660,"elapsed":3771,"user":{"displayName":"Dana Kulic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuEqV-iV8rSpDHO2jxqd7MnkIdtPYbssZsmpzQCw=s64","userId":"07057531030310424990"}},"outputId":"a0894d15-ebc0-42a7-b009-86ac52f4cade","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"]}],"source":["import sys\n","!{sys.executable} -m pip install gym"]},{"cell_type":"code","source":["!git clone https://github.com/Tobias-Fischer/RVSS2022.git"],"metadata":{"id":"vwGGniauG9RX","executionInfo":{"status":"ok","timestamp":1643770390260,"user_tz":-660,"elapsed":9148,"user":{"displayName":"Dana Kulic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuEqV-iV8rSpDHO2jxqd7MnkIdtPYbssZsmpzQCw=s64","userId":"07057531030310424990"}},"outputId":"3f76ca14-127f-44e0-8724-0a9ac7f7aeaf","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'RVSS2022'...\n","remote: Enumerating objects: 684, done.\u001b[K\n","remote: Counting objects: 100% (684/684), done.\u001b[K\n","remote: Compressing objects: 100% (551/551), done.\u001b[K\n","remote: Total 684 (delta 379), reused 356 (delta 125), pack-reused 0\u001b[K\n","Receiving objects: 100% (684/684), 39.30 MiB | 11.53 MiB/s, done.\n","Resolving deltas: 100% (379/379), done.\n"]}]},{"cell_type":"markdown","metadata":{"id":"wo1UIMwFGtVA"},"source":["#### Import dependencies"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"awbK78G1GtVA","executionInfo":{"status":"ok","timestamp":1643770404673,"user_tz":-660,"elapsed":780,"user":{"displayName":"Dana Kulic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuEqV-iV8rSpDHO2jxqd7MnkIdtPYbssZsmpzQCw=s64","userId":"07057531030310424990"}}},"outputs":[],"source":["# %matplotlib notebook\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","import numpy as np\n","import pickle\n","\n","import sys\n","import os\n","sys.path.insert(0, os.path.abspath('RVSS2022/Reinforcement_Learning/Support'))\n","\n","from gym_simple_gridworlds.envs.grid_env import GridEnv\n","from gym_simple_gridworlds.envs.grid_2dplot import *\n","from gym_simple_gridworlds.helper import *\n","\n","from IPython.core.display import display, HTML, Image"]},{"cell_type":"markdown","metadata":{"id":"wVcq_PYIGtVB"},"source":["# 2. Elements of an MDP (Grid World Example)\n","\n","Recall the grid in which our robot lives\n","\n","![](https://raw.githubusercontent.com/Tobias-Fischer/RVSS2022/main/Reinforcement_Learning/Support/images/GridWorldExample.png)"]},{"cell_type":"markdown","source":["\n","- The states $s \\in \\mathcal{S}$ correspond to locations in the grid. Each location has also a cell index associated to it, e.g., cell index 4 is associated to location (row=1,col=0)\n","- The robot can move up, down, left, or right. Actions correpond to unit increments or decrements in the specified direction.\n","    - Up : (-1,0)\n","    - Down: (1,0)\n","    - Left: (0,-1)\n","    - Right: (0, 1)\n","- Each action is represented by a number. Action (Up) is represented by 0, (Down) by 1, (Left) by 2 and, finally, (Right) by 3. No actions are available at a terminal state\n","\n","We have defined the class ``GridEnv`` to represent our Grid World MDP. **Take a look at the attributes of this class by placing the cursor somewhere on the class' name and hit SHIFT+TAB. If there's a + button at the top of the popup tooltip, this means the documentation spans a few lines, click it to show the full docstring, then scroll up.**"],"metadata":{"id":"gLPvOIGiMbl5"}},{"cell_type":"markdown","metadata":{"id":"w3oqJhvwGtVB"},"source":["## 2.1 Create Environment and Explore its Attributes\n","\n","The noise parameter corresponds to the probability of a change of direction when an action is taken (e.g., going left/right when agent decides to move up/down)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"OdO2o41sGtVC","executionInfo":{"status":"ok","timestamp":1643773261423,"user_tz":-660,"elapsed":301,"user":{"displayName":"Dana Kulic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuEqV-iV8rSpDHO2jxqd7MnkIdtPYbssZsmpzQCw=s64","userId":"07057531030310424990"}}},"outputs":[],"source":["# Create a Grid World instance\n","grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=-0.04)"]},{"cell_type":"markdown","metadata":{"id":"p4scjuAzGtVC"},"source":["### State and Action Spaces\n","\n","Let's take a look at the state and action spaces of our environment"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ed2JLqb5GtVC","executionInfo":{"status":"ok","timestamp":1643773269768,"user_tz":-660,"elapsed":343,"user":{"displayName":"Dana Kulic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuEqV-iV8rSpDHO2jxqd7MnkIdtPYbssZsmpzQCw=s64","userId":"07057531030310424990"}},"outputId":"312d56e4-5f5a-43e2-b758-9cc87b7cbad3","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Discrete(11)\n","[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n","\n","Discrete(4)\n","[0, 1, 2, 3]\n"]}],"source":["# State (or observation) space\n","print(grid_world.observation_space)\n","print(grid_world.get_states())\n","print()\n","\n","# Action space\n","print(grid_world.action_space)\n","print(grid_world.get_actions())"]},{"cell_type":"markdown","metadata":{"id":"4A6gWLd-GtVC"},"source":["### Transition Function\n","\n","Let's take a look at the current state transition function. Some things to keep in mind regarding the transition function:\n","\n","1. Given that $\\mathcal{T}: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow \\mathbb{R}$, the ``state_transitions`` attribute of the class ``GridEnv`` corresponds to a 3-Dimensional numpy array of size $11\\times4\\times11$.\n","2. With a noise attribute set to 0.2, at state 5, if the agent chooses to move up, it will end up at:\n","    - state 2 with $80\\%$ probability,\n","    - state 6 with $10\\%$ probability, or\n","    - state 5 with $10\\%$ probability"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"p168neVEGtVC","executionInfo":{"status":"ok","timestamp":1643773319534,"user_tz":-660,"elapsed":304,"user":{"displayName":"Dana Kulic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuEqV-iV8rSpDHO2jxqd7MnkIdtPYbssZsmpzQCw=s64","userId":"07057531030310424990"}},"outputId":"3a66bd15-3095-49b6-ed1a-6cf3d98eb88d","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.  0.  0.8 0.  0.  0.1 0.1 0.  0.  0.  0. ]\n"]}],"source":["print(grid_world.state_transitions[5,0])"]},{"cell_type":"markdown","metadata":{"id":"QzE47q6YGtVD"},"source":["### Living Reward and Reward Function\n","\n","Let's now take a quick look at the living reward (i.e., running cost) and reward function $\\mathcal{R}: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$.\n","\n","1. Living reward corresponds to the attribute ``living_rewards`` of the class ``GridEnv`` and is represented as an 1-Dimensional numpy array\n","2. The reward function corresponds to the attribute ``rewards`` of the class ``GridEnv`` and is also represented as a 2-Dimensional numpy array of size $11\\times4$"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"SbHMpqtXGtVD","executionInfo":{"status":"ok","timestamp":1643773365201,"user_tz":-660,"elapsed":471,"user":{"displayName":"Dana Kulic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuEqV-iV8rSpDHO2jxqd7MnkIdtPYbssZsmpzQCw=s64","userId":"07057531030310424990"}},"outputId":"23412683-4d9e-468a-dbd9-71a9727c7169","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Living rewards for all states:\n","[-0.04 -0.04 -0.04  1.   -0.04 -0.04 -1.   -0.04 -0.04 -0.04 -0.04]\n","\n"]}],"source":["# Living rewards\n","print(\"Living rewards for all states:\\n{}\\n\".format(grid_world.immediate_rewards))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"j_VKxox2GtVD"},"source":["### Policy\n","\n","Let's see the path and total reward of an agent moving on our grid world according to some random policy\n"]},{"cell_type":"code","execution_count":76,"metadata":{"id":"Xv6VtarzGtVD","executionInfo":{"status":"ok","timestamp":1643784593763,"user_tz":-660,"elapsed":463,"user":{"displayName":"Dana Kulic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuEqV-iV8rSpDHO2jxqd7MnkIdtPYbssZsmpzQCw=s64","userId":"07057531030310424990"}},"colab":{"base_uri":"https://localhost:8080/","height":333},"outputId":"21501025-ccb0-46c2-d624-aa4356988692"},"outputs":[{"output_type":"stream","name":"stdout","text":["Decoded policy\n"," [[ 3.  3.  3. -1.]\n"," [ 0. nan  0. -1.]\n"," [ 0.  2.  0.  2.]]\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 288x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}],"source":["# We randomly pick an action to take in each state\n","# Generate an initial random policy\n","#test_policy = define_random_policy(grid_world)\n","#encoded_test_policy = encode_policy(grid_world,test_policy)\n","#test_policy_matrix = decode_policy(grid_world,encoded_test_policy)\n","\n","\n","# Or, we could specify our own policy\n","test_policy_matrix = np.array([[3,      3,  3,  -1],\n","                          [0, np.NaN,  0,  -1],\n","                          [0,      2,  0,   2]])\n","encoded_test_policy = encode_policy(grid_world,test_policy_matrix)\n","\n","\n","plot_policy(grid_world,test_policy_matrix)\n","print(\"Decoded policy\\n\", test_policy_matrix)"]},{"cell_type":"markdown","metadata":{"id":"ot4aPblnGtVE"},"source":["Let's now apply this policy and observe the agent's behavior (blue dot in the figure shown below)."]},{"cell_type":"code","execution_count":77,"metadata":{"id":"cf-1PeP7GtVE","executionInfo":{"status":"ok","timestamp":1643784638496,"user_tz":-660,"elapsed":294,"user":{"displayName":"Dana Kulic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuEqV-iV8rSpDHO2jxqd7MnkIdtPYbssZsmpzQCw=s64","userId":"07057531030310424990"}},"outputId":"f60f4f91-f6a7-4769-b827-1a5953e7459d","colab":{"base_uri":"https://localhost:8080/","height":264}},"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPkAAAD3CAYAAADfRfLgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAIDUlEQVR4nO3dT4hV9xnG8efRcUwHnYQYx2AWjQtdhPyBMGJskk10UUvpLrRZB9y46CYE3BeEbESIGwm0i4B0242kcWEwRAkTQVBBwRZJBzR10mjCZMb583aRSypD7L2t58yZ+/j9rM6ZI8f3x/U7557jheuqEoBc67oeAEC7iBwIR+RAOCIHwhE5EG6k0ZONPV5jW55u8pRrxr2lZUnS6PrM34uPwvqqlrWs+a5HaYW1UQs3/3a7qrauPNZo5GNbntafTp9p8pRrxvFT1yRJhw7s6niSdjwK67v81Tnd2ni461FasW3+iG4dOXzjp45l/toG8CMiB8IRORCOyIFwRA6EI3IgHJED4YgcCEfkQDgiB8IRORCOyIFwRA6EI3IgHJED4YgcCEfkQDgiB8IRORCOyIFwRA6EI3IgHJED4YgcCEfkQDgiB8IRORCOyIFwRA6EI3IgHJED4YgcCNc3ctt7bH9m+1PbR1djKADNGeRKfkPSG1X1mqQJ2y+0PBOABo30+wNVdfO+3QVJS+2NA6BpA9+T235R0taqurLi5wdtT9meWlxcbHxAAA9noMhtPynpfUlvrzxWVSeqarKqJkdG+r4xALDKBnnwNiLpQ0nvrHjrDmAIDHIlf1PSbknv2T5je2/LMwFo0CAP3k5KOrkKswBoAR+GAcIRORCOyIFwRA6EI3IgHJED4YgcCEfkQDgiB8IRORCOyIFwRA6EI3IgHJED4YgcCEfkQDgiB8IRORCOyIFwRA6EI3IgHJED4YgcCEfkQDgiB8IRORCOyIFwRA6EI3IgHJED4fp+dfH/4t7Sso6futbkKdeM6ZlZSWJ9Q2p6Zlajyzu0bf5I16O0YnR5xwOPNRp5uu+/+06Xr1/seoxWjE7s0M82be56jFY9Pm89d6vrKdpxZUL68gHHGo18dP06HTqwq8lTrhnHT13T5esXdevk4a5HacW2t45o57N7o1+/p65f1J9DX7/fvnVElx5wjHtyIByRA+GIHAhH5EA4IgfCETkQjsiBcEQOhCNyIByRA+GIHAhH5EA4IgfCETkQjsiBcEQOhCNyIByRA+GIHAhH5EA4IgfCETkQjsiBcEQOhCNyIByRA+GIHAhH5EA4IgfCETkQjsiBcH0jt73d9gXbc7Yb/T5zAO0b5Er+taR9ks63PAuAFvS9MlfVnKQ526swDoCmPfQ9ue2DtqdsTy0uLjYxE4AGPXTkVXWiqiaranJkhFt2YK3h6ToQbpCn6xtsn5b0kqSPbO9pfywATRnkwduCpP2rMAuAFvB2HQhH5EA4IgfCETkQjsiBcEQOhCNyIByRA+GIHAhH5EA4IgfCETkQjsiBcEQOhCNyIByRA+GIHAhH5EA4IgfCETkQjsiBcEQOhCNyIByRA+GIHAhH5EA4IgfCETkQjsiBcEQOhHNVNXayx7bvrFff/aCx860l0zOzmv32ru599feuR2nF6MQOjW0e1zNbxroepRXTM7Pa8O1dPRf6+l2Z2KFLx373RVVNrjzW9/vJ8R9jm8e189m9XY/RiumZ2a5HaN3C5nHdDn39Fv7L69do5KPr1+nQgV1NnnLNOH7qmiSxviH1KKzv6gOOcU8OhCNyIByRA+GIHAhH5EA4IgfCETkQjsiBcEQOhCNyIByRA+GIHAhH5EA4IgfCETkQjsiBcEQOhCNyIByRA+GIHAhH5EA4IgfCETkQjsiBcEQOhCNyIByRA+GIHAhH5EA4IgfCETkQjsiBcANFbvuo7bO2j7U9EIBm9Y3c9suSNlXV65JGbe9ufywATRnkSv6KpI9726cl7W1vHABNGyTyJyTd7W3f6e3/yPZB21O2pxYXF5ueD8BDGiTyO5LGe9vjkr65/2BVnaiqyaqaHBkZaXo+AA9pkMjPSdrX294v6Xx74wBoWt/Iq+qCpDnbZyUtVdXn7Y8FoCkDvb+uqt+3PQiAdvBhGCAckQPhiBwIR+RAOCIHwhE5EI7IgXBEDoQjciAckQPhiBwIR+RAOCIHwhE5EI7IgXBEDoQjciAckQPhiBwIR+RAOCIHwhE5EI7IgXBEDoQjciAckQPhiBwIR+RAOCIHwhE5EM5V1djJHtu+s15994PGzreWTM/MSpKe2TLW8STtYH3DbXpmVlf/8Ksvqmpy5bFGI7f9T0k3Gjthf09Jur2Kf99qY33DbbXX9/Oq2rryh41GvtpsT/3Ub64UrG+4rZX1cU8OhCNyINywR36i6wFaxvqG25pY31DfkwPob9iv5AD6IHIg3NBGbvuo7bO2j3U9S9Nsb7d9wfac7ZGu52ma7T22P7P9qe2jXc/TNNvP99Z31vYfbbvLeYYyctsvS9pUVa9LGrW9u+uZGva1pH2Sznc9SEtuSHqjql6TNGH7ha4HatjVqvpF79+nJHX6f+VDGbmkVyR93Ns+LWlvh7M0rqrmqupfXc/Rlqq6WVVzvd0FSUtdztO0qlq4b3de0pddzSINb+RPSLrb277T28eQsf2ipK1VdaXrWZpm+ze2L0naJmmmy1mGNfI7ksZ72+OSvulwFvwfbD8p6X1Jb3c9Sxuq6i9V9bykf0j6dZezDGvk5/TDPask7VfuvWuk3sPEDyW9U1U3u56nabY33rd7V9L3Xc0iDWnkVXVB0pzts5KWqurzrmdqku0Ntk9LeknSR7b3dD1Tw96UtFvSe7bP2I56piLpl7Y/sf2Jfni7/tcuh+ETb0C4obySAxgckQPhiBwIR+RAOCIHwhE5EI7IgXD/Bo8npIh4XbHxAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 288x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}],"source":["# Create a Grid World instance\n","grid_world = GridEnv(gamma=0.99, noise=0.2, living_reward=-0.04)\n","s_x, s_y = get_state_to_plot(grid_world)\n","\n","# We can visualize our grid world using the render() function\n","fig, ax = grid_world.render()\n","agent, = ax.plot([], [], 'o', color='b', linewidth=6)\n","reward_text = ax.text(0.02, 0.95, '', transform=ax.transAxes)\n","\n","done = False\n","cumulative_reward = 0\n","grid_world.reset() #can reset to start from a different initial condition\n","cur_state = grid_world.cur_state\n","path_to_plot = []\n","\n","while not done:\n","    _, cur_reward, done, _ = grid_world.step(int(test_policy_matrix[cur_state[0], cur_state[1]]))\n","    cur_state = grid_world.cur_state\n","    n_x, n_y = get_state_to_plot(grid_world)\n","    cumulative_reward += cur_reward\n","    path_to_plot.append([cumulative_reward, n_x, n_y])\n","\n","def init():\n","    agent.set_data(s_x + 0.5, s_y + 0.5)\n","    reward_text.set_text('')\n","    return agent, reward_text\n","\n","def animate(i):\n","    if i < len(path_to_plot):\n","        r, n_x, n_y = path_to_plot[i]\n","        agent.set_data(n_x + 0.5, n_y + 0.5)\n","        reward_text.set_text('Cumulative reward: %.2f' % r)\n","    return agent, reward_text"]},{"cell_type":"code","source":["ani = animation.FuncAnimation(fig, animate, frames=len(path_to_plot), blit=False, interval=500, init_func=init,\n","                              repeat=False)\n","HTML(ani.to_html5_video())"],"metadata":{"id":"FnFtW4CTRNGm","executionInfo":{"status":"ok","timestamp":1643784643721,"user_tz":-660,"elapsed":309,"user":{"displayName":"Dana Kulic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuEqV-iV8rSpDHO2jxqd7MnkIdtPYbssZsmpzQCw=s64","userId":"07057531030310424990"}},"outputId":"d19f8fbe-7a97-4012-d2a9-5b4e6454a540","colab":{"base_uri":"https://localhost:8080/","height":309}},"execution_count":78,"outputs":[{"output_type":"execute_result","data":{"text/html":["<video width=\"288\" height=\"288\" controls autoplay>\n","  <source type=\"video/mp4\" src=\"data:video/mp4;base64,AAAAHGZ0eXBNNFYgAAACAGlzb21pc28yYXZjMQAAAAhmcmVlAAAOvG1kYXQAAAKtBgX//6ncRem9\n","5tlIt5Ys2CDZI+7veDI2NCAtIGNvcmUgMTUyIHIyODU0IGU5YTU5MDMgLSBILjI2NC9NUEVHLTQg\n","QVZDIGNvZGVjIC0gQ29weWxlZnQgMjAwMy0yMDE3IC0gaHR0cDovL3d3dy52aWRlb2xhbi5vcmcv\n","eDI2NC5odG1sIC0gb3B0aW9uczogY2FiYWM9MSByZWY9MyBkZWJsb2NrPTE6MDowIGFuYWx5c2U9\n","MHgzOjB4MTEzIG1lPWhleCBzdWJtZT03IHBzeT0xIHBzeV9yZD0xLjAwOjAuMDAgbWl4ZWRfcmVm\n","PTEgbWVfcmFuZ2U9MTYgY2hyb21hX21lPTEgdHJlbGxpcz0xIDh4OGRjdD0xIGNxbT0wIGRlYWR6\n","b25lPTIxLDExIGZhc3RfcHNraXA9MSBjaHJvbWFfcXBfb2Zmc2V0PS0yIHRocmVhZHM9MyBsb29r\n","YWhlYWRfdGhyZWFkcz0xIHNsaWNlZF90aHJlYWRzPTAgbnI9MCBkZWNpbWF0ZT0xIGludGVybGFj\n","ZWQ9MCBibHVyYXlfY29tcGF0PTAgY29uc3RyYWluZWRfaW50cmE9MCBiZnJhbWVzPTMgYl9weXJh\n","bWlkPTIgYl9hZGFwdD0xIGJfYmlhcz0wIGRpcmVjdD0xIHdlaWdodGI9MSBvcGVuX2dvcD0wIHdl\n","aWdodHA9MiBrZXlpbnQ9MjUwIGtleWludF9taW49MiBzY2VuZWN1dD00MCBpbnRyYV9yZWZyZXNo\n","PTAgcmNfbG9va2FoZWFkPTQwIHJjPWNyZiBtYnRyZWU9MSBjcmY9MjMuMCBxY29tcD0wLjYwIHFw\n","bWluPTAgcXBtYXg9NjkgcXBzdGVwPTQgaXBfcmF0aW89MS40MCBhcT0xOjEuMDAAgAAAClRliIQA\n","FP/+98dPwKbo+WbLnUU9ITCK2PpIKL1Y7NAydQkBKAiZDC9w8lN+JXCsXriw8J0AAJvwfgGd/5lT\n","ftKXjp7d+c5/FSJQIwV87sNKcXWnrnxpYd0Rba3AZfYRCF+6eGS3S3L2rMRlsbzZIwPk4PzDbW2Y\n","MCggwksMzIZdkNWdYGZ9cl5Hl8D27OYpD3+MeXNgQj8bs5AuKRjkPnIJXJkQdD/KbEr8epVIuVsT\n","vdgYVxmyMUJwbpiOEDNCSyNpuVUeddQUD8/h9SAPKlKzSnkkLI3CoI0cuU2RH15l+K8waGJFbrqF\n","b1oQG9HgBgItFsbNariH33y5/IRkEeKCkNdmMquII1h+oEhirrhgrn1uIblmxNbrmV+1jX1/VRLY\n","xkbci9HWqevDs52tpAta11tYhUdwRrzqZq+U6eCym0CB0lPj2oKkS3G+w0+b1xn8YVX/QuGrrKIv\n","v0MX/XaHlnXtFgxFxn4sEf22pT3PdyggmWVfeHFWEWP67r99Qx7yYscF4k+nC0AX3Ip30b3P/AOC\n","/J1k+Ez5kuX3YG+gLhhTSH2kMjZs1F/tRNzlN1IgzGJBYSdYO0E2cAl7npNTS8EdSURtn+hz3bc+\n","t+I1u5tz/gP1AuHzXd5jvFrBTRNQB/ZM+2oRbP9ZDEKyz1WN0n/5Y1PpTvfEj7qO4OMoiDO7nVbI\n","TVpvqRaB+cwG8fectGApf/CGpOsmhLH6vD0NkT/M3YP2o4cg5z6c/ObxpIaf2HDkNCwyvDMgWNFO\n","tvECoGLEmbtX+cWr+Mx8KOSv7h4GVcdXvBeijE2MvO0Uc0ELzWrD3KphlBr+rbCDKMPnQ+Epj9R1\n","GaSRC55UkhlEG28xj5hU0uxXkTyXWL1AKevyAIN3imefgyGN8+1V3YCiZXeZTdfsLs7ldgW+7S9d\n","CRqKXZS3GwycqFe6l3cEu9MVYXoFiXqjVjD0d6IM8F+XVRLhx9S2I0JBBAjGuX/rEApMc5ZwPN8P\n","tqhf6JLfXqVL3RmbYgZtJC2qeW1cWZDCoQiJWdtfubd7YfBMmvwTdutlpotHLpTU6++2HzYxfPQE\n","qWQl76/BvJmgV1DAjSU0IbyK5pFYTRcsWDqONYKNeJklKJYlhQQri2AaXkfCeY7Rvv1JLc3CwSL/\n","kvOFzLrdt5xXxz/IvaY0HwRlYz+21pVVIvs6+4pDdr0kk6uExj+mn5JsEPrs1vIR4UAo7no0SqDH\n","iIvdVmyHdpVbVVE3GIfgriRNk6IJn6/9LGKPk3IZ2ud0/B3FWPl3ZGPOtnMb2PHEUGd15pbxkeD+\n","z/qFf///9o+Z9rOv74QaPvNOz0z0ySOazPS3ITJTm5HJSBPF9dUM/wxH/Xo7OcvcHWD+4HsPiHrE\n","X+LzpC3ncAYzOEHkmwrSw2Q3YNZC1bFyQa/wah83igV9SKhi6tApw+gQ5kWnnIMSd+Ng38PYFdlK\n","Od1zInRR8yIEQg5DNy3D5MDooiBq0OhLS8zyCGEEF1Z+lZvnPSzRWxsqnYJgbEOcE86HnWSjsSTB\n","XZMx4Dk25LXsh58SDwEa2dy7AAAcMm2jGf9FSeOgViBV9dXTgDeCNruY4MBZiHfBMnmbInCgpKfx\n","XzU7NTcHy9JdHNso+h+BnrnfofSRy0eugaXA1jEgYnzs6QnlgFq9XZV5vC3NyxaOc3VQSUgvHT+l\n","eUvjrLICdldqU0L58wl1IHW0xfgquQ8rVQNP1Nalr5/K26jR/SSITJ692YkxPQOA6/2KuzwhOtu+\n","eEORiYqwtgZld9tLrd10q/DJ+/1FQk9rhLLKfuBw4Z/n0dMzfhgVlgipOIr3C0uaj6/ppWIG8APY\n","NOXUtjvuE+pvk44BwjmO9RI8ZaRkzqL1LHE9h2SxjND2thN8UXU10qu80CCvMQRmy+6CsqR4gaET\n","i6rkv4YiS49yeIi+IwFWtRjDqxQ2uEYol6KoNoxiuH4OA5VXZYGLhM6DLy3tdftjwDqeaj45JHWF\n","cw/DLUS7hkEhIAioSbTFeqWHrwu/ZWt4mGwX/h0+2L9rIgbqGvzLmR+esaA2fRq7y3t4hQ1iv6T/\n","N2gNgmGc8lbHr+7cpQcYOf2LciTlnHdY9xXa74DWV3mnjWAf6x7/9ki2nDct40aojd2Lr/Wdyr6R\n","9KxHzaYblpi326Vi/f9GSv1G7jlzSzT/CPlXNYdJNqjnlr6nVFZC/NvEG3IwsrbQzXppGzkav8NS\n","bItFTVD3X+Sjz+gBkj/JQ+bJABe7XoMzVV5UWMv9Cyl+2lTxuCNzans6QC5hOCO4hJ5V2vWyYSBl\n","ihRMYZpE3xPi0CQegxMZBHLjVdwCozB4YkRh4cZdj+6fYZuWki/I5h49aX3INIZuZruWe1CvkPIN\n","ZKlmZZ8VsXgdFtrTHTYYLhKDXSvrMyUwGY0l3/DU1c+qukFyhROUJ6evr9jiWvMI9c5e9prVzr6n\n","a1EDdk/j/GhCFubiwjmLtpOMvZrThhZdn/r/9x/6X/2RqiZ1l/+FNyPD1tb5/LhhAs+Hc3Vkss5S\n","iDg1gHbZWNfQYgp5QclSKMBxCpXm7+c2SlsxcwKP/obMa05qcQyY2jUuyTLkax4OzHj7n1lqTXKI\n","z5GV0gmjxx98wWQ3PUdgYQdTBna/4V09SnYr7WL3Ngd7sO0kEHiLx0FY+31KHOpltC869Ad4uEQt\n","ll5fDVgPQSpWi4/OkV6VWmNsnB4l/omLVEs9j8EvnTyGUvpKqRjihRijkiXBIqmmF0c6Cv6ObDsw\n","mtVFkE+PlHTArbvBI5MYc2OLPvxZClpZyi679B0deFc5Km0wAJ6gUo9T/iK1dc5IWKTB1n9Z8vBP\n","tDS63JSY/2a8Dlcq4TtjiTsT3nUVFTsO1BYHLw5bc3Wd/uELZSdB69zY0R9dX2pFIFAP2vie15Kj\n","WhZU5zcLCJhINkxPwUQRTKkaKd273WHWy3qUaES1qt71ibxoW7MI8skoFbHADsALP7RAcLxeTxhs\n","eGTOQXfQyvJg3qHI4CZHLV2YxAAAbKCBXxszmhNxZrXbpXInAVWwqfw4bd183/gQmnbijDBr5Svp\n","2e/XtFlG8mEuXv2KFnXva3gi3DzK7GkHZ3oPTrp7WvtsgQaFej1rW/TET62bMmVaCevDCPdDxKTD\n","/mQOVpuIiGNLWex2F1kFUupqKsvpLqrwfgd4/HC5hVX1eBM4RQhb+/VJrPDmfs+8jDd1OscZ4KTJ\n","SelsOMJtD9W4B+ddysiJKJdA52g6JcWadS31pTyrDdz7gKhja/tG0RB/xVnPZNq21Fm5PufY8X5e\n","jAq+jQUlnE2Wx89eWBvAiNocRZiBX9a3c//V0jvdIDtJgo7OtGIE9UHQTTjqTLJQLE0vSlbJfRhN\n","6RKz0PVurHr6IHRSCfiRqZw4J+pHjceVfsKCl/MJ3/D7yV57PRKguDZ7BkxCpLxX1I6EvQvmbIJ4\n","WlyDb+MF8ctl7EHvRLkbqIGzXl9Gc/L10eKuvjjS0/N2Ii5X8GjjFpTrJsrWXtBR0uT6hKUQzELk\n","PJLWcYkaum8JQnjjY5j9GIQxAAABp0GaIWxBL/61KoB+t5sAG0t3+ibSBBILDOWE+Z+l2tAU+SmM\n","iRSJl6A1vhQrV3qgkJCOzJSSM8JriQ+O8wB3MRplMoZge1gZwVG17UsfxqxaCgt/YNk+xVizA2tk\n","d3wLNi+vUrQ2XiLHpBNHwhqPZ92Zb2UA525MY/o0q+hufsSqul4EUHgDehxpfuagBrfFgO9ixk+n\n","Tdf1jwIYCrkcWJRDO56qA2S2lO2BryMzA1u+dwz+rLCj3xOVezp4lA5hZeobAedxnTTgRARDYwyn\n","0mSNWgatkWeYKMGpb2wfsaKOibVzTHOya3RGTlnU6z4hDF+Vu5KcPvStvwwNXQxp+Pfm9mF1pvAB\n","qux8nlSExI9gryEo7FNbst2bSNTgzmhZrkKjL9tmkohtIAcA4yMsV4CPxGUfpQUXkryeoCjglE3D\n","gfig6U4myPxXzEZ3PMfQD3gQ4HVAxKlsTRk1eUm+WvWyWuF1xwfJ3pRYJpcGi2FwLjZQs9oSbvMs\n","sdG2vRP8jCqWRfZIZz5/R4jEerVjD8N8HYGQuXSyPoQkJoP5N9riowuYyLqewAAAAyZtb292AAAA\n","bG12aGQAAAAAAAAAAAAAAAAAAAPoAAAD6AABAAABAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAA\n","AAEAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAACUHRyYWsA\n","AABcdGtoZAAAAAMAAAAAAAAAAAAAAAEAAAAAAAAD6AAAAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAA\n","AAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAABIAAAASAAAAAAACRlZHRzAAAAHGVsc3QAAAAAAAAA\n","AQAAA+gAAAAAAAEAAAAAAchtZGlhAAAAIG1kaGQAAAAAAAAAAAAAAAAAAEAAAABAAFXEAAAAAAAt\n","aGRscgAAAAAAAAAAdmlkZQAAAAAAAAAAAAAAAFZpZGVvSGFuZGxlcgAAAAFzbWluZgAAABR2bWhk\n","AAAAAQAAAAAAAAAAAAAAJGRpbmYAAAAcZHJlZgAAAAAAAAABAAAADHVybCAAAAABAAABM3N0YmwA\n","AACzc3RzZAAAAAAAAAABAAAAo2F2YzEAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAABIAEgAEgAAABI\n","AAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAY//8AAAAxYXZjQwFkAAz/\n","4QAYZ2QADKzZQSCWhAAAAwAEAAADABA8UKZYAQAGaOvjyyLAAAAAHHV1aWRraEDyXyRPxbo5pRvP\n","AyPzAAAAAAAAABhzdHRzAAAAAAAAAAEAAAACAAAgAAAAABRzdHNzAAAAAAAAAAEAAAABAAAAHHN0\n","c2MAAAAAAAAAAQAAAAEAAAACAAAAAQAAABxzdHN6AAAAAAAAAAAAAAACAAANCQAAAasAAAAUc3Rj\n","bwAAAAAAAAABAAAALAAAAGJ1ZHRhAAAAWm1ldGEAAAAAAAAAIWhkbHIAAAAAAAAAAG1kaXJhcHBs\n","AAAAAAAAAAAAAAAALWlsc3QAAAAlqXRvbwAAAB1kYXRhAAAAAQAAAABMYXZmNTcuODMuMTAw\n","\">\n","  Your browser does not support the video tag.\n","</video>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"execution_count":78}]},{"cell_type":"markdown","metadata":{"id":"hhQHAGepGtVE"},"source":["### 2.2 Summary and Code Correspondance\n","\n","$\\gamma$\n","\n","<table style=\"width:20%\">\n","<thead>\n","<tr>\n","<th style=\"width:150px;font-size:20px;text-align:center;\">MDP Notation (from notes)</th>\n","<th colspan=2 style=\"width:250px;font-size:20px;text-align:center;\">GridEnv</th>\n","</tr>\n","    <tr>\n","        <td></td>\n","        <th style=\"font-size:15px;text-align:center;\">Class attribute</th>\n","        <th style=\"font-size:15px;text-align:center;\">Class method</th>\n","    </tr>\n","</thead>\n","<tbody>\n","<tr>\n","    <td style=\"text-align:left;\">$\\mathcal{S} = \\{0,\\dots,10\\}$</td>\n","    <td style=\"text-align:left;\">observation_space</td>\n","    <td style=\"text-align:left;\">get_states()</td>\n","</tr>\n","<tr>\n","    <td style=\"text-align:left;\">$\\mathcal{A} = \\{\\text{up}, \\text{down}, \\text{left}, \\text{right} \\}$</td>\n","    <td style=\"text-align:left;\">action_space</td>\n","    <td style=\"text-align:left;\">get_actions()</td>\n","</tr>\n","<tr>\n","    <td style=\"text-align:left;\">$\\mathcal{T}(s, a, s')$</td>\n","    <td style=\"text-align:left;\">state_transitions</td>\n","    <td></td>\n","</tr>\n","<!-- <tr>\n","    <td style=\"text-align:left;\">$\\mathcal{R}(s)$</td>\n","    <td style=\"text-align:left;\">immediate_rewards</td>\n","    <td></td>\n","</tr>-->\n","    <tr>\n","    <td style=\"text-align:left;\">$\\mathcal{R}(s,a)$</td>\n","    <td style=\"text-align:left;\">rewards</td>\n","    <td></td>\n","</tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"1hMDvZSgGtVE"},"source":["# 3. Iterative Policy Evaluation\n","\n","Recall the definition of the iterative policy evaluation algorithm\n","\n","![](https://raw.githubusercontent.com/Tobias-Fischer/RVSS2022/main/Reinforcement_Learning/Support/images/IterativePolicyEvaluation.png)"]},{"cell_type":"markdown","source":["Let's now compute the value function of the same policy $\\pi$\n","\n","![](https://raw.githubusercontent.com/Tobias-Fischer/RVSS2022/main/Reinforcement_Learning/Support/images/example_policy.png)"],"metadata":{"id":"EAuWEUj8NKrN"}},{"cell_type":"markdown","source":["We consider a grid world environment with the following attributes:\n","- Discount factor $\\gamma = 0.99$ (class attribute ``gamma=0.99``)\n","- Stochastic transition matrix (class attribute ``noise=0.2``)\n","- A non-zero living cost and big rewards are obtained at terminal states (class attribute ``living_reward=-0.04``)\n","\n","We have defined the helper function ``encode_policy()`` to encode the policy $\\pi$ shown in the image above. The return variable ``policy_pi`` is a dictionary of dictionaries, where each element corresponds to the probability of selecting an action $a$ at a given state $s$\n","\n","Keep in mind that each action is represented by a number. Action (Up) is represented by 0, (Down) by 1, (Left) by 2 and, finally, (Right) by 3."],"metadata":{"id":"t40A0zgbNK9Y"}},{"cell_type":"code","execution_count":79,"metadata":{"id":"JMy1n30nGtVF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643784655334,"user_tz":-660,"elapsed":392,"user":{"displayName":"Dana Kulic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuEqV-iV8rSpDHO2jxqd7MnkIdtPYbssZsmpzQCw=s64","userId":"07057531030310424990"}},"outputId":"98280554-b354-4498-8728-6c8d1a527163"},"outputs":[{"output_type":"stream","name":"stdout","text":["Action probabilities at state 0 are:\n","{0: 0.0, 1: 0.0, 2: 0.0, 3: 1.0}\n"]}],"source":["grid_world = GridEnv(gamma=0.99, noise=0.2, living_reward=-0.04)\n","policy_pi = encoded_test_policy\n","\n","print(\"Action probabilities at state 0 are:\\n{}\".format(dict(policy_pi[2])))"]},{"cell_type":"markdown","metadata":{"id":"5Y5WupMJGtVF"},"source":["Given the policy $\\pi$, let's now compute its state-value function using iterative policy evaluation.\n","\n","**TODO**: \n","Complete the computation of value function update for each state. We have decomposed this computation into 2 steps:\n","\n","1. Compute discounted sum of state values of all successor states: $\\text{discounted_v} = \\gamma\\sum_{s' \\in \\mathcal{S}}\\mathcal{T}(s,a,s')v(s')$ for each action\n","\n","\n","2. Compute expectation over all actions: $\\sum_{a \\in \\mathcal{A}}\\pi(a|s)(\\mathcal{R}(s,a) + \\text{discounted_v})$ \n","\n","\n","**Keep in Mind**: Correspondance between the mathematical notation and implemented code\n","\n","\n","<table style=\"width:20%\">\n","<thead>\n","<tr>\n","<th style=\"width:150px;font-size:20px;text-align:center;\">Notation in Slides</th>\n","<th colspan=2, style=\"width:450px;font-size:20px;text-align:center;\">Code</th>\n","</tr>\n","    <tr>\n","        <td></td>\n","        <th style=\"font-size:15px;text-align:left;\">Variable/Attribute</th>\n","        <th style=\"font-size:15px;text-align:left;\">Type</th>\n","    </tr>\n","</thead>\n","<tbody>\n","<tr>\n","    <td style=\"text-align:left;\">$\\gamma$</td>\n","    <td style=\"text-align:left;font-size:15px;\">grid_world.gamma</td>\n","    <td>float</td>\n","</tr>\n","<tr>\n","    <td style=\"text-align:left;\">$\\mathcal{T}(s, a, s')$</td>\n","    <td style=\"text-align:left;font-size:15px;\">grid_world.state_transitions[idx_s, idx_a, idx_s]</td>\n","    <td>numpy 3d-array</td>\n","</tr>\n","<tr>\n","    <td style=\"text-align:left;\">$\\mathcal{R}(s, a)$</td>\n","    <td style=\"text-align:left;font-size:15px;\">grid_world.rewards[idx_s, idx_a]</td>\n","    <td>numpy 2d-array</td>\n","</tr>\n","<tr>\n","    <td style=\"text-align:left;\">$\\pi(a|s)$</td>\n","    <td style=\"text-align:left;font-size:15px;\">policy_pi[idx_s][idx_a]</td>\n","    <td>dict of dict</td>\n","</tr>\n","<tr>\n","    <td style=\"text-align:left;\">$v_\\pi(s)$</td>\n","    <td style=\"text-align:left;font-size:15px;\">v[idx_s]</td>\n","    <td>dict</td>\n","</tr>\n","</table>"]},{"cell_type":"code","execution_count":85,"metadata":{"id":"IVu1D1gRGtVF","colab":{"base_uri":"https://localhost:8080/","height":281},"executionInfo":{"status":"ok","timestamp":1643784955360,"user_tz":-660,"elapsed":311,"user":{"displayName":"Dana Kulic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuEqV-iV8rSpDHO2jxqd7MnkIdtPYbssZsmpzQCw=s64","userId":"07057531030310424990"}},"outputId":"86a5d2db-ac54-407d-a3bd-5ae3e10cbed7"},"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0}\n"]},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPkAAAD3CAYAAADfRfLgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQoklEQVR4nO3df3DUd53H8eebbBKEQKD82FANKT2N/kHBtlSo549hN7mJgbkZ2oKn/nFmOq1jR7kyeON4f1yif1xH/rikM45gZ6DjjNPeyI3M2KSkdaVCObE9QaM3tl7QJgZNQvkhbcBsspvP/dEYCQWznvvNd/Pm9fgrm+8ny/vd7ZPNLpmJhRAQEb/mxT2AiERLkYs4p8hFnFPkIs4pchHnEkW9swXVoWp5TTHvsmRkcxMAVCZ8/r14U+yXn6A8Nx73KJEYT5QzevbX50IIK669VtTIq5bXcOqFZ4t5lyVjV2c/AO1b62KeJBo3w35XXv45j37763GPEomOHY+QeerR/utd8/nXtohMUeQizilyEecUuYhzilzEOUUu4pwiF3FOkYs4p8hFnFPkIs4pchHnFLmIc4pcxDlFLuKcIhdxTpGLOKfIRZxT5CLOKXIR5xS5iHOKXMQ5RS7inCIXcU6RizinyEWcU+QizilyEedKNvKjR4/S0NjI5lSKffv2ve16Npvl8zt3sjmV4r777+fMmTNT1/bu3cvmVIqGxkaOHTs2m2MXxPNu4H+/jvwQn8r9ikdyfde9HkLgG/mzPJR7jc/l+jgdRqeufX/iEg/lXuOh3Gt8f+LSrMxbkpHn83na2to4sH8/z3V380xnJ729vdPOHDx4kOrqal44coSWlha+umcPAL29vXR2ddF9+DBPHjhAa2sr+Xw+jjWuy/Nu4H8/gIZ5i/ly2TtveP3H4TK/Y4wnym7jc2VJvp4/C8CbIc9TExf497LVtJet5qmJC4yE6Pcrych7enqoq6tj9erVVFRUsHXLFjKZzLQzmUyG+7ZtA+BjTU2cOHGCEAKZTIatW7ZQWVlJbW0tdXV19PT0xLHGdXneDfzvB7DWFrCIshtefylcJmWLMTPeZ+/gMnkuhBynwmXutAUssjKqrIw7bQEnw+XI5y3JyIeHh1m1atXU7ZqaGoaHh6edGbrqTCKRYFFVFRcvXizoa+PkeTfwv18hzpNjuZVP3V5mCc6Tm/x84m2fj1pJRi4ixTNj5Ga20cx+aGbHzax9NoZKJpMMDg5O3R4aGiKZTE47U3PVmVwux5sjIyxdurSgr42T593A/36FWEaCc2F86vb5kGMZicnP5972+agV8kzeD6RCCB8CVprZHRHPxLp16+jr72dgYICxsTE6u7pIp9PTzqTTab5z6BAAh7u7uXfTJsyMdDpNZ1cX2WyWgYEB+vr7Wb9+fdQjF8zzbuB/v0JstIUcCW8QQuDV8AcWMI9bLMFdtpCfhMuMhDwjIc9PwmXusoWRzzPjXyMhhKGrbo4Dkb8dmEgkaG1t5dMtLUzk8zywfTv19fW0d3Rwx9q1NDQ0sGPHDnbv3s3mVIolS5bweEcHAPX19TQ3N9PU1ERZIkFbWxtlZTd+k2S2ed4N/O8HsCc/yM/DFd4gzz/mfs2n5i0jRwCged4SNthCfhwu81C+j0qMR8tqAFhkZXx83jJ25X8DwD/MW8Yii34/CyEUdtBsHfBYCGHLNZ9/GHgYYGHN7Xf/7PjzRR+yFOzq7AegfWtdzJNE42bY78rLP+fRb3897lEi0bHjETJPPXoyhLDh2msFvfFmZrcAXwMevPZaCOGJEMKGEMKGRHn0ry9E5C9TyBtvCeBbwBeu+dZdROaAQp7JtwP3AHvM7Admdm/EM4lIERXyxtvTwNOzMIuIREA/DCPinCIXcU6RizinyEWcU+QizilyEecUuYhzilzEOUUu4pwiF3FOkYs4p8hFnFPkIs4pchHnFLmIc4pcxDlFLuKcIhdxTpGLOKfIRZxT5CLOKXIR5xS5iHOKXMQ5RS7inCIXcU6RizinyEWcU+QizilyEecshFC0O3vHre8Jf/fFbxTt/krJ6fNZAN69rDLmSaJxM+w38cZl3vX6b+MeJRJnVryTVx/ffjKEsOHaazP+fnL5k5E3R3jpdE/cY0SiYuUaqqoWxT1GpLIV76D/lvq4x4jEePnEDa8VNfLKxDzat9YV8y5Lxq7Ofl463cPw01+Ke5RIJD/xGO9Z8wHXj98rp7Os7FsQ9yiROHvblRte02tyEecUuYhzilzEOUUu4pwiF3FOkYs4p8hFnFPkIs4pchHnFLmIc4pcxDlFLuKcIhdxTpGLOKfIRZxT5CLOKXIR5xS5iHOKXMQ5RS7inCIXcU6RizinyEWcU+QizilyEecUuYhzilzEuZKN/OjRozQ0NrI5lWLfvn1vu57NZvn8zp1sTqW47/77OXPmzNS1vXv3sjmVoqGxkWPHjs3m2MLN89hdGennZ8cf4YfPNvLbX/3HDc+NXhmk5/hnOXnkk7x68stMTIwDMJEf49WTX+bkkU/Sc/yzjF4ZjGTOkow8n8/T1tbGgf37ea67m2c6O+nt7Z125uDBg1RXV/PCkSO0tLTw1T17AOjt7aWzq4vuw4d58sABWltbyefzcaxxU7qZHrtE+WLWrN3JO2//+J891/fKN7h1zQPcnXqKRHkVw795FoDhgWdJlFdxd+opbl3zAH2vPBHJnCUZeU9PD3V1daxevZqKigq2btlCJpOZdiaTyXDftm0AfKypiRMnThBCIJPJsHXLFiorK6mtraWuro6eHp+/brgU3UyPXUXlUhYteR9mZTc8E0Lg0rlTLF/1UQBW1jZxYfg4ABeG/4uVtU0ALF/1US6dO0kIoehzlmTkw8PDrFq1aup2TU0Nw8PD084MXXUmkUiwqKqKixcvFvS1Eh09dtPlxi+RKK/C5r31W8Ir569gbPR1AMZGX6dy/goAbF6CRHkVufFLRZ9hxsjN7FYzO2Vmo2ZW1N9nLiLRK+SZ/AKQBn4U8SxTkskkg4N/ehNiaGiIZDI57UzNVWdyuRxvjoywdOnSgr5WouP9sRvsO8RPjz3IT489SHb03IznE+XV5MZHCBM5ALKjr1Mx+exdMX8F2cln9TCRIzc+QqK8uugzzxh5CGE0hHCx6H/yn7Fu3Tr6+vsZGBhgbGyMzq4u0un0tDPpdJrvHDoEwOHubu7dtAkzI51O09nVRTabZWBggL7+ftavXz+b49/UvD92q27bxvs/sp/3f2Q/lfOXz3jezKhefifnBo8CcHagm1uSfwvALckPcnagG4Bzg0epXn4XZlb0mf/qb7/N7GHgYYCFNbf/1QPBW6/TWltb+XRLCxP5PA9s3059fT3tHR3csXYtDQ0N7Nixg927d7M5lWLJkiU83tEBQH19Pc3NzTQ1NVGWSNDW1kZZ2Y3fGJHiupkeu7HR8/Qc/wz53BXA+N1r/8mdH/0mifKF/OKlL/I36/+ZyvnLue19n+GXp77Cb365n4XV7yFZ2wxAsraZ//3pv3HyyCdJlC/mvXf9ayRzWqHv5pnZD4CGEELuRmeWrH5vOPXCs0UarbTs6uznpZdeZvjpL8U9SiSSn3iMjRs/QPvWurhHicSuzn5eOZ1lZd+CuEeJxNnbrtD7teaTIYQN114ryXfXRaR4Cnl3vdzMMsB64Dkz2xj9WCJSLDO+Jg8hjAMNszCLiERA366LOKfIRZxT5CLOKXIR5xS5iHOKXMQ5RS7inCIXcU6RizinyEWcU+QizilyEecUuYhzilzEOUUu4pwiF3FOkYs4p8hFnFPkIs4pchHnFLmIc4pcxDlFLuKcIhdxTpGLOKfIRZxT5CLOKXIR5xS5iHOKXMS5GX918V8im5tgV2d/Me+yZJw+n6Vi5RqSn3gs7lEiUbFyDafPZ10/fuPzJzh725W4R4nE+PyJG14rauTeLVq0mHev+UDcY0Ti9PkshLiniNaC+fN497sq4x4jEqfPZ294raiRVybm0b61rph3WTL++Ayn/eamm2G/X9zgml6TizinyEWcU+QizilyEecUuYhzilzEOUUu4pwiF3FOkYs4p8hFnFPkIs4pchHnFLmIc4pcxDlFLuKcIhdxTpGLOKfIRZxT5CLOKXIR5xS5iHOKXMQ5RS7inCIXcU6RizinyEWcU+QizpVs5EePHqWhsZHNqRT79u172/VsNsvnd+5kcyrFffffz5kzZ6au7d27l82pFA2NjRw7dmw2xy6I591A+5XafiUZeT6fp62tjQP79/NcdzfPdHbS29s77czBgweprq7mhSNHaGlp4at79gDQ29tLZ1cX3YcP8+SBA7S2tpLP5+NY47o87wbaD0pvv5KMvKenh7q6OlavXk1FRQVbt2whk8lMO5PJZLhv2zYAPtbUxIkTJwghkMlk2LplC5WVldTW1lJXV0dPT08ca1yX591A+0Hp7VeSkQ8PD7Nq1aqp2zU1NQwPD087M3TVmUQiwaKqKi5evFjQ18bJ826g/aD09ivJyEWkeAqK3MzazexFM3s86oEAkskkg4ODU7eHhoZIJpPTztRcdSaXy/HmyAhLly4t6Gvj5Hk30H5QevvNGLmZ3QVUhRA+DFSY2T1RD7Vu3Tr6+vsZGBhgbGyMzq4u0un0tDPpdJrvHDoEwOHubu7dtAkzI51O09nVRTabZWBggL7+ftavXx/1yAXzvBtoPyi9/RIFnNkEfG/y4wxwL/DfkU3EW69jWltb+XRLCxP5PA9s3059fT3tHR3csXYtDQ0N7Nixg927d7M5lWLJkiU83tEBQH19Pc3NzTQ1NVGWSNDW1kZZWVmU4/5FPO8G2q8U97MQwp8/YPYvwKkQQreZNQAfDCF85arrDwMPAyysuf3unx1/Psp5Y7Orsx+A9q11MU8SDe03t+3q7Oe7j6ZPhhA2XHutkNfkl4DFkx8vBn5/9cUQwhMhhA0hhA2J8kK+MRCR2VRI5CeAP77oaAB+FN04IlJsM0YeQjgFjJrZi0A+hPBy9GOJSLEU9P11COGfoh5ERKKhH4YRcU6RizinyEWcU+QizilyEecUuYhzilzEOUUu4pwiF3FOkYs4p8hFnFPkIs4pchHnFLmIc4pcxDlFLuKcIhdxTpGLOKfIRZxT5CLOKXIR5xS5iHOKXMQ5RS7inCIXcU6RizinyEWcU+QizilyEecUuYhzBf3q4kJlcxPs6uwv5l2WjNPnswDab466Wfa7HgshFO0PMrPXgdn8r7gcODeLf95s035z22zvVxdCWHHtJ4sa+Wwzsx+HEDbEPUdUtN/cVir76TW5iHOKXMS5uR75E3EPEDHtN7eVxH5z+jW5iMxsrj+Ti8gMFLmIc3M2cjNrN7MXzezxuGcpNjO71cxOmdmomRX1B5ZKgZltNLMfmtlxM2uPe55iM7O1k/u9aGZPmpnFOc+cjNzM7gKqQggfBirM7J64ZyqyC0Aa+FHcg0SkH0iFED4ErDSzO+IeqMh+GUL44OT/nwCx/lv5nIwc2AR8b/LjDHBvjLMUXQhhNIRwMe45ohJCGAohjE7eHAfycc5TbCGE8atuZoGBuGaBuRv5EuCNyY8vTd6WOcbM1gErQgi/iHuWYjOzvzez/wGSwPk4Z5mrkV8CFk9+vBj4fYyzyP+Dmd0CfA14MO5ZohBC+G4IYS1wBtga5yxzNfITvPWaFaABv69dXZp8M/FbwBdCCENxz1NsZlZ51c03gD/ENQvM0chDCKeAUTN7EciHEF6Oe6ZiMrNyM8sA64HnzGxj3DMV2XbgHmCPmf3AzFy9pwI0mdlRMzvKW9+uPx/nMPqJNxHn5uQzuYgUTpGLOKfIRZxT5CLOKXIR5xS5iHOKXMS5/wM8O6uNHT3eQgAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 288x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}],"source":["def policy_evaluation(grid_env, policy, plot=False, threshold=0.00001):\n","    \n","    \"\"\"\n","    This function computes the value function for a policy pi in a given environment grid_env.\n","    \n","    :param grid_env (GridEnv): MDP environment\n","    :param policy (dict - stochastic form): Policy being evaluated\n","    :return: (dict) State-values for all non-terminal states\n","    \"\"\"\n","        \n","    # Obtain list of all states in environment\n","    v = {s: 0.0 for s in grid_env.get_states()}\n","    theta = threshold\n","    delta = 1000\n","\n","    while delta > theta:\n","        delta = 0.0\n","        # For all states\n","        for s in v.keys():\n","\n","            old_v = v[s]\n","            new_v = 0\n","\n","            # For all actions\n","            for a in grid_env.get_actions():\n","                discounted_v = 0\n","\n","                # For all states that are reachable from s with action a\n","                for s_next in grid_env.get_states():\n","                    # TODO 1: Compute discounted sum of state values for all successor states\n","                    discounted_v += 0\n","\n","                    \n","                # TODO 2: Compute expectation over all actions\n","                new_v += 0\n","\n","            v[s] = new_v\n","            delta = max(delta, np.abs(old_v - new_v))\n","\n","    if plot:\n","        plot_value_function(grid_env, v)\n","        \n","    return v\n","        \n","        \n","# Call the policy evalution function\n","v = policy_evaluation(grid_world, policy_pi, plot=True)\n","print(v)"]},{"cell_type":"markdown","metadata":{"id":"xtO7Uk-nGtVF"},"source":["# 4. Policy Iteration"]},{"cell_type":"markdown","metadata":{"id":"RQUzao_mGtVG"},"source":["Recall the definition of the policy iteration algorithm\n","\n","![](https://raw.githubusercontent.com/Tobias-Fischer/RVSS2022/main/Reinforcement_Learning/Support/images/PolicyIteration.png)"]},{"cell_type":"markdown","source":["Starting with a random policy, let's find the optimal policy for a grid world environment with attributes:\n","\n","We consider a grid world environment with the following attributes:\n","- Discount factor $\\gamma = 0.99$ (class attribute ``gamma=0.99``)\n","- Stochastic transition matrix (class attribute ``noise=0.2``)\n","- Rewards are only obtained at terminal states (class attribute ``living_reward=-0.04``)\n","\n","We will first define some helper methods:\n","- ``one_step_look_ahead(grid_env, state, value_function)``, this method computes the action-value function for a state $s$ given the state-value function $v$. This corresponds to $\\mathcal{R}(s,a) + \\gamma\\sum_{s' \\in \\mathcal{S}}\\mathcal{T}(s,a,s')v_\\pi(s')\\, \\forall \\, a \\in \\mathcal{A}$\n","\n","\n","- ``update_policy(grid_world, policy, value_function)``, this method updates the current policy $\\pi$ given the state-value function $v$ by taking the action $a$ with the highest action-value. \n","\n","\n","- ``define_random_policy(grid_env)`` in script ``helper.py``, this method generates a random policy for environment ``grid_env``"],"metadata":{"id":"GtDsTrEjNtsl"}},{"cell_type":"code","execution_count":81,"metadata":{"id":"0lgB6hMuGtVG","executionInfo":{"status":"ok","timestamp":1643784675108,"user_tz":-660,"elapsed":285,"user":{"displayName":"Dana Kulic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuEqV-iV8rSpDHO2jxqd7MnkIdtPYbssZsmpzQCw=s64","userId":"07057531030310424990"}}},"outputs":[],"source":["def one_step_look_ahead(grid_env, state, value_function):\n","    \"\"\"\n","     Compute the action-value function for a state $s$ given the state-value function $v$.\n","     \n","     :param grid_env (GridEnv): MDP environment\n","     :param state (int): state for which we are looking one action ahead\n","     :param value_function (dict): state-value function associated to a given policy py\n","     \n","     :return: (np.array) Action-value function for all actions available at state s\n","    \"\"\"\n","    action_values = []\n","    \n","    for action in grid_env.get_actions():\n","        discounted_value = 0\n","        for s_next in grid_env.get_states():\n","             discounted_value += grid_env.state_transitions[state, action, s_next] * value_function[s_next]\n","        \n","        q_a = grid_env.rewards[state, action] + grid_env.gamma * discounted_value\n","        action_values.append(q_a)\n","    \n","    return np.array(action_values)"]},{"cell_type":"code","execution_count":82,"metadata":{"id":"FsnGxnvtGtVG","executionInfo":{"status":"ok","timestamp":1643784677815,"user_tz":-660,"elapsed":309,"user":{"displayName":"Dana Kulic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuEqV-iV8rSpDHO2jxqd7MnkIdtPYbssZsmpzQCw=s64","userId":"07057531030310424990"}}},"outputs":[],"source":["def update_policy(grid_env, cur_policy, value_function):\n","    \"\"\"\n","     Update a given policy based on a given value_function\n","     \n","     :param grid_env (GridEnv): MDP environment\n","     :param cur_policy (matrix form): Policy to update\n","     :param value_function: state-value function associated to a policy cur_policy\n","     \n","     :return: (dict) Updated policy\n","    \"\"\"\n","    \n","    states = grid_env.get_states(exclude_terminal=True)\n","    \n","    for s in states:\n","        # Obtain state-action values for state s using the helper function one_step_look_ahead\n","        action_values = one_step_look_ahead(grid_env, s, value_function)\n","        \n","        # Find (row, col) coordinates of cell with index s\n","        row, col = np.argwhere(grid_env.grid == s)[0]\n","        \n","        cur_policy[row, col] = np.argmax(action_values)\n","        \n","    return cur_policy"]},{"cell_type":"markdown","metadata":{"id":"-4WkoHtbGtVG"},"source":["Let's now define the policy iteration core algorithm.\n","\n","**TODO**: Complete the main steps of the policy iteration algoritm.\n","- Use ``policy_evaluation(.)`` to compute the state-value function of a given policy\n","- Use ``update_policy(.)`` to obtain an updated policy"]},{"cell_type":"code","execution_count":86,"metadata":{"id":"-1XJB-z5GtVG","executionInfo":{"status":"ok","timestamp":1643784973429,"user_tz":-660,"elapsed":302,"user":{"displayName":"Dana Kulic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuEqV-iV8rSpDHO2jxqd7MnkIdtPYbssZsmpzQCw=s64","userId":"07057531030310424990"}}},"outputs":[],"source":["def policy_iteration(grid_env, policy, plot=False):\n","    \"\"\"\n","    This function iteratively updates a given policy pi for a given environment grid_env until convergence to optimal policy\n","    \n","    :param grid_env (GridEnv): MDP environment\n","    :param policy (matrix from): Deteministic policy being updated\n","    :return: (dict) State-values for all non-terminal states\n","    \"\"\"\n","    prev_policy = np.zeros(policy.shape)\n","    \n","    while not np.all(np.equal(prev_policy, policy)):\n","        \n","        # Encode policy. This policy representation is needed for policy evaluation\n","        encoded_policy = encode_policy(grid_env, policy)\n","        # Set prev_policy to current policy\n","        prev_policy = policy.copy()\n","        \n","        #TODO: Complete the remaining steps\n","        # 1. Evaluate the given policy (policy_evaluation expects an mdp and the enconded_policy as arguments)\n","        \n","        # 2. Update policy using helper function update_policy\n","        \n","        \n","    if plot:\n","        plot_policy(grid_env, policy)\n","    \n","    return policy"]},{"cell_type":"code","execution_count":87,"metadata":{"id":"MFrBvPSsGtVH","colab":{"base_uri":"https://localhost:8080/","height":510},"executionInfo":{"status":"ok","timestamp":1643784980949,"user_tz":-660,"elapsed":990,"user":{"displayName":"Dana Kulic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuEqV-iV8rSpDHO2jxqd7MnkIdtPYbssZsmpzQCw=s64","userId":"07057531030310424990"}},"outputId":"aaf32f24-bd15-45e4-d1f4-b6b916f23c5d"},"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 288x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 288x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}],"source":["# Create a grid world mdp\n","grid_world = GridEnv(gamma=0.99, noise=0.2, living_reward=-0.04)\n","\n","# Generate an initial random policy\n","initial_policy = define_random_policy(grid_world)\n","plot_policy(grid_world,initial_policy)\n","\n","# Compute optimal policy using policy iteration\n","optimal_policy = policy_iteration(grid_world, initial_policy, plot=True)"]}],"metadata":{"kernelspec":{"display_name":"IntelligentRobotics","language":"python","name":"intelligentrobotics"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"IntroRLv2.ipynb","provenance":[{"file_id":"https://github.com/Tobias-Fischer/RVSS2022/blob/main/Reinforcement_Learning/Session%201%20-%20IntroRL.ipynb","timestamp":1643683024622}],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}
